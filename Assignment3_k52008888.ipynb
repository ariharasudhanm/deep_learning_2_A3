{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use only.\n",
    "Any reproduction of this manuscript,\n",
    "no matter whether as a whole or in parts,\n",
    "no matter whether in printed or in electronic form,\n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 3 - SS 2023 -->\n",
    "\n",
    "# From Reversing Convolutions to VAEs (14 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains one of the assignments for the exercises in Deep Learning and Neural Nets 2.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility functions that should work without (too much) problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless explicitly allowed!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will tackle the topic of auto-encoders,\n",
    "a simple trick to use neural networks for unsupervised learning.\n",
    "Whereas auto-encoders with fully connected layers are pretty simple,\n",
    "new types of layers are necessary to build convolutional auto-encoders.\n",
    "The most important of these new layers is the transposed convolution,\n",
    "which is often confusingly referred to as a deconvolution.\n",
    "Finally, we turn auto-encoders into generative models with some clever tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import inspect\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "torch.manual_seed(1806)\n",
    "torch.cuda.manual_seed(1806)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.pytorch\n"
     ]
    }
   ],
   "source": [
    "# google colab data management\n",
    "import os.path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    _home = 'gdrive/MyDrive/'\n",
    "except ImportError:\n",
    "    _home = '~'\n",
    "finally:\n",
    "    data_root = os.path.join(_home, '.pytorch')\n",
    "\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AutoMNIST(datasets.MNIST):\n",
    "    \"\"\" Wrapper around MNIST for training auto-encoders. \"\"\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index].numpy()\n",
    "        img = Image.fromarray(x, mode='L')\n",
    "        target = Image.fromarray(x, mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self):\n",
    "        return os.path.join(self.root, 'MNIST', 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self):\n",
    "        return os.path.join(self.root, 'MNIST', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_image_data(url: str, md5: str = None, size: int = 224):\n",
    "    \"\"\"\n",
    "    Download and pre-process some image for AlexNet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to the image.\n",
    "    md5 : str, optional\n",
    "        MD5 checksum of the image.\n",
    "    size : int, optional\n",
    "        Expected size for the image after cropping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : (1, 3, size, size) torch.Tensor\n",
    "        Ready-to-predict image.\n",
    "    \"\"\"\n",
    "    from torchvision.datasets.utils import download_url\n",
    "    path = os.path.expanduser(data_root)\n",
    "    download_url(url, path, md5=md5)\n",
    "    file_name = url.rsplit('/', maxsplit=1)[-1]\n",
    "    im = Image.open(os.path.join(path, file_name))\n",
    "    # constants taken from torchvision.models docs\n",
    "    normalise = transforms.Compose([\n",
    "        transforms.RandomCrop(size=size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.485, .456, .406), (.229, .224, .225))\n",
    "    ])\n",
    "    return normalise(im).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_to_image(*data: torch.Tensor, \n",
    "                  means: tuple = (0, ), stds: tuple = (1., )) -> Image:\n",
    "    \"\"\"\n",
    "    Convert multiple tensors to one big image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data0, data1, ... dataN : torch.Tensor\n",
    "        One or more tensors to be merged into a single image.\n",
    "    means : tuple or torch.Tensor, optional\n",
    "        Original mean of the image before normalisation.\n",
    "    stds : tuple or torch.Tensor, optional\n",
    "        Original standard deviation of the image before normalisation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image : Image\n",
    "        PIL image with all of the tensors next to each other.\n",
    "    \"\"\"\n",
    "    # concatenate all data\n",
    "    big_pic = torch.cat([x for x in data], dim=-1)\n",
    "    \n",
    "    means = torch.tensor(means)\n",
    "    stds = torch.tensor(stds)\n",
    "    to_image = transforms.Compose([\n",
    "        # inverts normalisation of image\n",
    "        transforms.Normalize(-means / stds, 1. / stds),\n",
    "        transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    \n",
    "    return to_image(big_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularisedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for networks that expose state for regularisation.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    To train an auto-encoder with some norm-regularisation (untested):\n",
    "    >>> ae = AutoEncoder(784)\n",
    "    >>> mse = RegularisedLoss(nn.MSELoss(), torch.norm)\n",
    "    >>> x = torch.randn(1, 784)\n",
    "    >>> logits, code = ae.forward(x)\n",
    "    >>> mse((logits, code), x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_func: nn.Module = None, reg_func: nn.Module = None,\n",
    "                 weight: float = 1.):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_func : nn.Module\n",
    "            Standard loss function computed from logits and targets.\n",
    "        reg_func : nn.Module, optional\n",
    "            Regularisation function computed from exposed network state.\n",
    "        weight : float, optional\n",
    "            Weighting factor for the regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.regulariser = reg_func\n",
    "        self.weight = weight\n",
    "        self._regularisation_cache = []\n",
    "        \n",
    "    def split_loss(self, total_loss):\n",
    "        \"\"\"\n",
    "        Split total loss values in original loss and regularisation part.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        total_loss : list or float\n",
    "            One or more loss values computed with this function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        orig_loss : Tensor\n",
    "            One ore more loss values from the original part of the loss.\n",
    "        reg_loss : Tensor\n",
    "            One or more loss values from the regularsiation part of the loss.\n",
    "        \"\"\"\n",
    "        if len(self._regularisation_cache) == 0:\n",
    "            raise ValueError(\"no regularisation loss yet, make sure to call forward first\")\n",
    "            \n",
    "        _totals = torch.tensor(total_loss)\n",
    "        reg_loss = torch.tensor(self._regularisation_cache[-_totals.numel():])\n",
    "        orig_loss = _totals - self.weight * reg_loss\n",
    "        return orig_loss, reg_loss\n",
    "        \n",
    "    def forward(self, outputs, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : (Tensor, Tensor)\n",
    "            Tuple consisting of logits and exposed state.\n",
    "        y : Tensor\n",
    "            Targets for the original loss part.\n",
    "        \"\"\"\n",
    "        logits, state = outputs\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        if self.regulariser is not None:\n",
    "            reg_loss = self.regulariser(state)\n",
    "            self._regularisation_cache.append(reg_loss.item())\n",
    "            loss = loss + self.weight * reg_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AutoEncoderTrainer:\n",
    "\n",
    "    def __init__(self,\n",
    "         model: nn.Module,\n",
    "         criterion: nn.Module,\n",
    "         optimiser: optim.Optimizer,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Module\n",
    "            Neural Network that will be trained.\n",
    "        criterion : torch.nn.Module\n",
    "            Loss function to use for training.\n",
    "        optimiser : torch.optim.Optimizer\n",
    "            Optimisation strategy for training.\n",
    "        tracker : Tracker, optional\n",
    "            Tracker to keep track of training progress.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimiser = optimiser\n",
    "        \n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\" Current state of learning. \"\"\"\n",
    "        return {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"objective\": self.criterion.state_dict(),\n",
    "            \"optimiser\": self.optimiser.state_dict(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\" Device of the (first) model parameters. \"\"\"\n",
    "        return next(self.model.parameters()).device\n",
    "\n",
    "    def _forward(self, data: DataLoader, metric: callable):\n",
    "        device = self.device\n",
    "\n",
    "        for x, y in data:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = self.model(x)\n",
    "            # print(logits.size()) \n",
    "            res = metric(logits, y)\n",
    "            yield res\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, data: DataLoader, metric: callable) -> list:\n",
    "        self.model.eval()\n",
    "        losses = [res.item() for res in self._forward(data, metric)]\n",
    "        # print(len(losses))\n",
    "        return losses\n",
    "\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def update(self, data: DataLoader) -> list:\n",
    "        opt = self.optimiser\n",
    "        self.model.train()\n",
    "\n",
    "        errs = []\n",
    "        for err in self._forward(data, self.criterion):\n",
    "            errs.append(err.item())\n",
    "\n",
    "            opt.zero_grad()\n",
    "            err.backward()\n",
    "            opt.step()\n",
    "\n",
    "        return errs\n",
    "    \n",
    "    def _print_errs(self, epoch: int, errs : list):\n",
    "        \"\"\"\n",
    "        Print error summary for current epoch.\n",
    "\n",
    "        If possible, the loss is additionally decomposed in\n",
    "        reconstruction and regularisation loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Rank of the current epoch.\n",
    "        errs : list\n",
    "            The errors to summarise.\n",
    "        \"\"\"\n",
    "        print(f\"Epoch {epoch: 2d} - avg loss: {sum(errs) / len(errs):.6f}\", end=' ')\n",
    "        try:\n",
    "            rec_errs, reg_errs = self.criterion.split_loss(errs)\n",
    "            print(f\"(rec: {sum(rec_errs) / len(errs):.4f}, reg: {sum(reg_errs) / len(errs):.4f})\")\n",
    "        except (AttributeError, ValueError):\n",
    "            print()\n",
    "\n",
    "\n",
    "    def _display_result(self, xs: torch.Tensor, count: int = 10):\n",
    "        \"\"\"\n",
    "        Visualise a number of reconstructions from an auto-encoder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xs : torch.Tensor\n",
    "            Batch of images to be reconstructed.\n",
    "            The batch is assumed to have the correct shape\n",
    "            for feeding it to the network.\n",
    "        count : int, optional\n",
    "            Number of samples in the batch to visualise.\n",
    "\n",
    "        \"\"\"\n",
    "        xs = xs[:count]\n",
    "        # get reconstructions\n",
    "        preds = self.model.reconstruct(xs.to(self.device)).cpu()\n",
    "\n",
    "        # convert to images\n",
    "        xs = xs.view(-1, 1, 28, 28)  # unflatten\n",
    "        x_im = data_to_image(*xs, means=(.1307, ), stds=(.3081, ))\n",
    "        preds = preds.view(-1, 1, 28, 28)\n",
    "        res_im = data_to_image(*preds)\n",
    "\n",
    "        # paste together\n",
    "        im = Image.new('L', (len(xs) * 28, 56))\n",
    "        im.paste(x_im, (0, 0))\n",
    "        im.paste(res_im, (0, 28))\n",
    "        display(im, metadata={'width': '100%'})\n",
    "\n",
    "\n",
    "    def train(self, loader: DataLoader, num_epochs: int = 10, vis_every: int = 5):\n",
    "        \"\"\"\n",
    "        Train an auto-encoder for a number of epochs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loader : DataLoader\n",
    "            A data loader for iterating over batches of the data.\n",
    "        num_epochs : int, optional\n",
    "            Number of times to iterate the dataset.\n",
    "        vis_every : int, optional\n",
    "            Frequency, during training, of \n",
    "            intermediate visualisation of reconstructions.\n",
    "        \"\"\"\n",
    "        # take random batch for visualising reconstructions\n",
    "        ref_inputs, _ = next(iter(loader))\n",
    "\n",
    "        # evaluate random performance\n",
    "        errs = self.evaluate(loader, self.criterion)\n",
    "        self._print_errs(0, errs)\n",
    "        self._display_result(ref_inputs)\n",
    "\n",
    "        # train for some epochs\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            errs = self.update(loader)\n",
    "            self._print_errs(0, errs)\n",
    "\n",
    "            if epoch % vis_every == 0:\n",
    "                self._display_result(ref_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Auto-Encoders\n",
    "\n",
    "An easy way to use supervised models in an unsupervised setting \n",
    "is to invent a prediction task that only requires the inputs.\n",
    "The most straightforward approach to this paradigm is to learn the identity function.\n",
    "It is trivial to find a network that solves this task perfectly (linear regression).\n",
    "In order to get more interesting models,\n",
    "it is therefore important to make it slightly harder for the network to learn the identity.\n",
    "\n",
    "The auto-encoder is a neural network architecture for learning these identity functions.\n",
    "In general, auto-encoders consist of two parts:\n",
    "the first part is the **encoder**, which maps the inputs to some *code*\n",
    "and the second part is the **decoder**, which maps this code back to the inputs.\n",
    "This setup is especially interesting when the code is much smaller than the input.\n",
    "In this case, the code forms a bottleneck for the information flow,\n",
    "and the network must learn to compress the information in the inputs to get good reconstructions.\n",
    "This effectively allows to learn a lossy compression scheme where\n",
    "the encoder can be used to compress the inputs and the decoder is used for decompression.\n",
    "\n",
    "Typically, encoder and decoder will have some sort of symmetry in their architecture.\n",
    "In fully-connected models, this symmetry can be obtained by transposing the weight matrices.\n",
    "Convolutional layers can be transposed as well, although that might not be as obvious.\n",
    "Note that we are only talking about the architecture, i.e. the shape of the weight matrices,\n",
    "and not about the weights, which are typically **not shared** between encoder and decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <figure style=\"display: inline-block; width: 49%; margin: 0\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" />\n",
    "    <figcaption style=\"width: 100%;\"> Normal convolution </figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; width: 49%; margin: 0\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif\" />\n",
    "    <figcaption style=\"width: 100%; text-align: center;\"> Transposed convolution </figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "*visualisations taken from the [github](https://github.com/vdumoulin/conv_arithmetic) that comes with [this guide](https://arxiv.org/abs/1603.07285)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Exercise 1: Transposed Convolutions (3 points)\n",
    "\n",
    "Although the transpose of a matrix is typically not its inverse,\n",
    "it can be interpreted as a way to *undo* a matrix multiplication.\n",
    "This is especially true for fully connected layers,\n",
    "where we find the tranpose of the weight matrix in the backward pass.\n",
    "Using this analogy, we can find a transposed convolution\n",
    "by taking the operation we find in the backward pass of a conv layer.\n",
    "\n",
    "To get a feeling for how transposed convolutions can *undo* convolutions,\n",
    "we are going to be implementing a method from Zeiler and Fergus (from ZF net).\n",
    "They used transposed convolutions to visualise activations in the input space.\n",
    "However, we will also need to undo max-pooling layers and ReLU non-linearities.\n",
    "Therefore, we first focus on how to undo different kinds of layers.\n",
    "An example of how to undo a ReLU non-linearity is provided in `UndoableReLU`.\n",
    "\n",
    " > Implement the `undo` methods of the `UndoableMaxPool2d` and `UndoableConv2d` classes below.\n",
    " > As the title suggests, convolutions can be undone by transposed convolutions.\n",
    " > Pooling operations can generally be undone by upsampling operations.\n",
    " > However, for max-pooling, we can get better results\n",
    " > if we keep track of the indices in the original inputs.\n",
    " > Do not forget about the different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfaf2c9aaa77b57fc6198f3ebbb2f15a",
     "grade": false,
     "grade_id": "cell-628cae5c125d2303",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UndoableReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a relu layer that adds 'undo' functionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, module: nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.original = module\n",
    "        \n",
    "    def forward(self, s: torch.Tensor):\n",
    "        return torch.relu(s), None\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def undo(self, a: torch.Tensor, context):\n",
    "        return torch.relu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0fbca9652f4f6b14229a224b1e759af",
     "grade": false,
     "grade_id": "cell-1fb1254336e8ab34",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UndoableMaxPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a max-pooling layer that adds 'undo' functionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, module: nn.MaxPool2d):\n",
    "        super().__init__()\n",
    "        self.original = module\n",
    "        \n",
    "    @property\n",
    "    def kwargs(self):\n",
    "        return {\n",
    "            k: getattr(self.original, k) \n",
    "            for k in (\"kernel_size\", \"stride\", \"padding\")\n",
    "        }\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # NOTE: the code below is equivalent to\n",
    "        # out, indices = nn.functional.max_pool2d(\n",
    "        #     x, return_indices=True,\n",
    "        #     kernel_size=self.original.kernel_size,\n",
    "        #     stride=self.original.stride,\n",
    "        #     padding=self.original.padding,\n",
    "        # )\n",
    "        out, indices = nn.functional.max_pool2d(\n",
    "            x, return_indices=True, **self.kwargs\n",
    "        )\n",
    "        return out, (indices, x.shape[-2:])\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def undo(self, y: torch.Tensor, context):\n",
    "        \"\"\"\n",
    "        Undo the operation from the forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : torch.Tensor\n",
    "            Data with same shape as the output of the original module.\n",
    "        context\n",
    "            Additional information to correctly undo the forward operation.\n",
    "            This is typically the second term in the output of `forward`.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        indices, shape = context\n",
    "        return nn.functional.max_unpool2d(y, indices, **self.kwargs, output_size=shape)\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21ea3f66db73587f4c2a7ad4e097066a",
     "grade": true,
     "grade_id": "cell-c9725dd49fb17067",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(1, 3, 32, 32)\n",
    "pool = nn.MaxPool2d(kernel_size=2)\n",
    "wrapped = UndoableMaxPool2d(pool)\n",
    "out, context = wrapped(x)\n",
    "assert torch.allclose(out, pool(x))\n",
    "assert wrapped.undo(out, context).shape == x.shape, (\n",
    "    \"ex1: MaxPool2d.undo produces incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f35120c644d32ba4ddc2a922775126d",
     "grade": true,
     "grade_id": "cell-9df9f45dfb216f0d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(1, 3, 32, 32)\n",
    "pool = nn.MaxPool2d(kernel_size=3)\n",
    "wrapped = UndoableMaxPool2d(pool)\n",
    "out, context = wrapped(x)\n",
    "assert torch.allclose(out, pool(x))\n",
    "assert wrapped.undo(out, context).shape == x.shape, (\n",
    "    \"ex1: MaxPool2d.undo does not (correctly) pad outputs (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dbba6fcbbb41a7d5fcf19c916a0b793",
     "grade": false,
     "grade_id": "cell-81b9c193f35af782",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UndoableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a convolutional layer that adds 'undo' functionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, module: nn.Conv2d):\n",
    "        super().__init__()\n",
    "        self.original = module\n",
    "        \n",
    "    @property\n",
    "    def kwargs(self):\n",
    "        return {\n",
    "            k: getattr(self.original, k) \n",
    "            for k in (\"padding\", \"stride\", \"dilation\", \"groups\")\n",
    "        }\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return nn.functional.conv2d(\n",
    "            x, self.original.weight, self.original.bias, **self.kwargs\n",
    "        ), x.shape[-2:]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def undo(self, s: torch.Tensor, context):\n",
    "        \"\"\"\n",
    "        Undo the operation from the forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s : torch.Tensor\n",
    "            Data with same shape as the output of the original module.\n",
    "        context\n",
    "            Additional information to correctly undo the forward operation.\n",
    "            This is typically the second term in the output of `forward`.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        out_pad = context[0] -1 - (self.original.dilation[0])*(self.original.kernel_size[0]-1) - (s.shape[2] -1) *self.original.stride[0]\n",
    "        result =nn.functional.conv_transpose2d(s, self.original.weight, **self.kwargs, output_padding=out_pad)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2ccafbf532deb492bee671ea71cce39",
     "grade": true,
     "grade_id": "cell-3483ccc444d20372",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(1, 3, 32, 32)\n",
    "conv = nn.Conv2d(3, 16, 7)\n",
    "wrapped = UndoableConv2d(conv)\n",
    "out, context = wrapped(x)\n",
    "assert torch.allclose(out, conv(x)) and context == (32, 32)\n",
    "assert wrapped.undo(out, context).shape == x.shape, (\n",
    "    \"ex1: Conv2d.undo produces incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2ef5c42665b29a5c7e6f62e7e7158ad",
     "grade": true,
     "grade_id": "cell-a44463f88b494dc3",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(1, 3, 32, 32)\n",
    "conv = nn.Conv2d(3, 16, 7, stride=2)\n",
    "wrapped = UndoableConv2d(conv)\n",
    "assert wrapped.undo(*wrapped(x)).shape == x.shape, (\n",
    "    \"ex1: Conv2d.undo does not (correctly) pad outputs (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c30132b912dd51286e2834947c4702ac",
     "grade": true,
     "grade_id": "cell-9b4f26be10132b40",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97a67cf04f966bdb4b695c4bef7bb4ed",
     "grade": true,
     "grade_id": "cell-757d05a0e86e847b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Visualising Features (1 point)\n",
    "\n",
    "In the previous exercise I mentioned Zeiler and Fergus.\n",
    "They used this kind of undoable modules to visualise features.\n",
    "I think it is worth trying this out for yourself.\n",
    "Is there anything we can learn from these visualisations?\n",
    "\n",
    " > Implement the `visualise_activations` method of `UndoableSequential`.\n",
    " > The `wrapped` attribute gives you the undoable versions of each layer.\n",
    " > This way, you should be able to compute activations with context.\n",
    " > These activations with context can eventually be *undone* to obtain visualisations.\n",
    " > Rather than visualising all activations at once, however,\n",
    " > the goal should be to keep the `k` highest values and set other values to zero.\n",
    " > You can use the `keep_top_k` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdb418a51d4628affe7fc1d86a12062c",
     "grade": false,
     "grade_id": "cell-2feca7f07595b984",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def keep_top_k(tensor: torch.Tensor, k: int = 9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Drop all but the K highest entries in a tensor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : torch.Tensor\n",
    "        The tensor to start from.\n",
    "    k : int\n",
    "        The number of entries to keep.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : torch.Tensor\n",
    "        A tensor with only the K highest entries of `tensor`\n",
    "        in the corresponding positions.\n",
    "        All other entries are zero.\n",
    "    \"\"\"\n",
    "    result = torch.zeros_like(tensor)\n",
    "    vals, indices = torch.flatten(tensor, start_dim=1).topk(k, dim=-1)\n",
    "    torch.flatten(result, start_dim=1).scatter_(-1, indices, vals)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bb9d8af5356aae5a14c5eaa2a50c1ff",
     "grade": false,
     "grade_id": "cell-cc29165d7d1028d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UndoableSequential(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a sequential layer that adds 'undo' functionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    UNDOABLE_MODULES = {\n",
    "        nn.Conv2d: UndoableConv2d,\n",
    "        nn.MaxPool2d: UndoableMaxPool2d,\n",
    "        nn.ReLU: UndoableReLU,\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, module: nn.Sequential):\n",
    "        super().__init__()\n",
    "        self.original = module\n",
    "        self.wrapped = nn.ModuleList([\n",
    "            self.UNDOABLE_MODULES[type(m)](m) for m in module\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, until: int = None):\n",
    "        context = []\n",
    "        for module in self.wrapped[:until]:\n",
    "            x, ctx = module(x)\n",
    "            context.append(ctx)\n",
    "        return x, context\n",
    "    \n",
    "    def visualise_activations(self, x: torch.Tensor, layer: int = None, k: int = 9):\n",
    "        \"\"\"\n",
    "        Visualise activations in input space.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            The data for which the activations should be visualised.\n",
    "        layer : int, optional\n",
    "            The layer to visualise the activations for.\n",
    "            By default, activations in the last layer are visualised.\n",
    "        k : int, optional\n",
    "            The number of activations to visualise.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        visualisation : torch.Tensor\n",
    "            The visualisations with the same shape as `x`.\n",
    "        \"\"\"\n",
    "        if layer <= 0:\n",
    "            return x\n",
    "        if layer > len(self.original):\n",
    "            raise ValueError(f\"no layer '{layer}' in a {len(self.original)}-layer network\")\n",
    "            \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        activations = x\n",
    "        for i in range(layer):\n",
    "            activations, _ = self.wrapped[i](activations)\n",
    "        return keep_top_k(activations, k=k)\n",
    "    \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d6a226dc3294ca1cb38c4bbd4c5c488",
     "grade": false,
     "grade_id": "cell-5c7ae15d4d5dffbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/ari/.pytorch/cat.jpg\n"
     ]
    }
   ],
   "source": [
    "# download some random 256x256 kitten image\n",
    "kitten_url = 'https://homes.cs.washington.edu/~moreau/media/vta/cat.jpg'\n",
    "kitten_md5 = 'b852db33459ff958c9bba4e015cb37eb'\n",
    "x_im = download_image_data(kitten_url, kitten_md5)\n",
    "\n",
    "# download trained network\n",
    "net = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6f0af105715ffc1d0ae1674d45d1bba",
     "grade": true,
     "grade_id": "cell-970c27b2f58a4220",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ex2: first layer vis does not have correct shape (-0.5 points)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m seq \u001b[39m=\u001b[39m UndoableSequential(net\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m      3\u001b[0m vis_layer1 \u001b[39m=\u001b[39m seq\u001b[39m.\u001b[39mvisualise_activations(x_im, layer\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[39massert\u001b[39;00m vis_layer1\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m x_im\u001b[39m.\u001b[39mshape, (\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mex2: first layer vis does not have correct shape (-0.5 points)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mcount_nonzero(vis_layer1\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1089\u001b[39m, (\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mex2: too many non-zero values in first layer vis for k=9 (-0.5 points)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: ex2: first layer vis does not have correct shape (-0.5 points)"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "seq = UndoableSequential(net.features)\n",
    "vis_layer1 = seq.visualise_activations(x_im, layer=1)\n",
    "\n",
    "assert vis_layer1.shape == x_im.shape, (\n",
    "    \"ex2: first layer vis does not have correct shape (-0.5 points)\"\n",
    ")\n",
    "assert torch.count_nonzero(vis_layer1.sum(1)) <= 1089, (\n",
    "    \"ex2: too many non-zero values in first layer vis for k=9 (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97bf63d8eca515d9c0eadb08b41b9f58",
     "grade": true,
     "grade_id": "cell-7076e3389bc18ca9",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ex2: fourth layer vis does not have correct shape (-0.5 points)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test Cell: do not edit or delete!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vis_layer2 \u001b[39m=\u001b[39m seq\u001b[39m.\u001b[39mvisualise_activations(x_im, layer\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[39massert\u001b[39;00m vis_layer2\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m x_im\u001b[39m.\u001b[39mshape, (\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mex2: fourth layer vis does not have correct shape (-0.5 points)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mcount_nonzero(vis_layer2\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m24336\u001b[39m, (\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mex2: too many non-zero values in fourth layer vis for k=9 (-0.5 points)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: ex2: fourth layer vis does not have correct shape (-0.5 points)"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "vis_layer2 = seq.visualise_activations(x_im, layer=4)\n",
    "assert vis_layer2.shape == x_im.shape, (\n",
    "    \"ex2: fourth layer vis does not have correct shape (-0.5 points)\"\n",
    ")\n",
    "assert torch.count_nonzero(vis_layer2.sum(1)) <= 24336, (\n",
    "    \"ex2: too many non-zero values in fourth layer vis for k=9 (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "606ecd7db17af54f5cbb0b4e55cb8ae9",
     "grade": false,
     "grade_id": "cell-35ddcf5eed890a9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 64 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# sanity check\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m out \u001b[39m=\u001b[39m data_to_image(vis_layer1[\u001b[39m0\u001b[39;49m], x_im[\u001b[39m0\u001b[39;49m], vis_layer2[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      3\u001b[0m                     means\u001b[39m=\u001b[39;49m(\u001b[39m.485\u001b[39;49m, \u001b[39m.456\u001b[39;49m, \u001b[39m.406\u001b[39;49m), stds\u001b[39m=\u001b[39;49m(\u001b[39m.229\u001b[39;49m, \u001b[39m.224\u001b[39;49m, \u001b[39m.225\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m display(out, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m100\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m})\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mdata_to_image\u001b[0;34m(means, stds, *data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mConvert multiple tensors to one big image.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m    PIL image with all of the tensors next to each other.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# concatenate all data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m big_pic \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m data], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m means \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(means)\n\u001b[1;32m     24\u001b[0m stds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(stds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 64 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "out = data_to_image(vis_layer1[0], x_im[0], vis_layer2[0],\n",
    "                    means=(.485, .456, .406), stds=(.229, .224, .225))\n",
    "display(out, metadata={'width': '100%'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Exercise 3: Convolutional Auto-Encoders (3 points)\n",
    "\n",
    "Architecturally an auto-encoder is not much more than a model \n",
    "where inputs and outputs have the same dimensions.\n",
    "For fully-connected networks this is relatively straightforward.\n",
    "However, with our newly acquired knowledge on transposed convolutions,\n",
    "also convolutional and pooling layers should not pose too much problems.\n",
    "One key difference is that the transposed convolutions n an auto-encoder\n",
    "will have their own learnable parameters.\n",
    "\n",
    "The architecture is typically symmetrical and consists of two parts:\n",
    "\n",
    " 1. an **encoder** that transforms the image to some latent space and\n",
    " 2. a **decoder** that produces an image from vectors in the latent space.\n",
    " \n",
    "Since both components can operate independently from each other,\n",
    "auto-encoders can be used to e.g. learn compression algorithms.\n",
    "Because the output of both networks can be useful, \n",
    "it is typically a good idea to have them predict logits,\n",
    "i.e. not to use activation functions at the end.\n",
    "This way, both the output of the encoder\n",
    "and the output of the decoder can be used in loss functions.\n",
    "\n",
    " > Implement the `AutoEncoder` class with an `encoder` and a `decoder` network.\n",
    " > The `encoder` network should have at least \n",
    " > one average-pooling and two convolutional layers.\n",
    " > The `decoder` architecture should be symmetric to that of the `encoder`,\n",
    " > except for the exact placement of activation functions.\n",
    " > The `forward` method should aim to reconstruct the inputs\n",
    " > using the `encoder` and `decoder` networks.\n",
    " > You can ignore the `to_intensities` and `reconstruct` methods (for now).\n",
    " \n",
    "**Hint:** Average pooling can be *undone* by upsampling the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2718e157247be4aa80fddb78e0b2a00",
     "grade": false,
     "grade_id": "cell-c67df72970121511",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\" Convolutional auto-encoder. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, hid_channels: int, code_channels: int, \n",
    "                 kernel_size: int, stride: int = 1, pooling: int = 2, \n",
    "                 activation: nn.Module = nn.ReLU(), assumption: str = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of channels in the inputs.\n",
    "        hid_channels : int\n",
    "            Number of channels in the hidden layer(s).\n",
    "        code_channels : int\n",
    "            Number of channels in the latent space.\n",
    "        kernel_size : int or tuple\n",
    "            Window size for convolutional layers\n",
    "        stride : int or tuple, optional\n",
    "            Window strides for convolutional layers.\n",
    "        pooling : int or tuple, optional\n",
    "            Window size for the average pooling.\n",
    "        activation : nn.Module, optional\n",
    "            Activation function for the auto-encoder.\n",
    "        assumption : str, optional\n",
    "            The assumption used for reconstruction, available options are:\n",
    "            \n",
    "            - gauss: assume targets to be standard normally distributed\n",
    "            - truncated: assume targets to be Gaussian truncated to [0, 1]\n",
    "            - binomial: assume targets to be probabilities for binomial\n",
    "            - binary: assume targets to be binomially distributed\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.assumption = assumption\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.decoder = nn.Sequential()\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        self.encoder.add_module('conv1', nn.Conv2d(in_channels, hid_channels, kernel_size, stride))\n",
    "        self.encoder.add_module('average pooling', nn.AvgPool2d(kernel_size=pooling))\n",
    "        self.encoder.add_module('activation', activation)\n",
    "        self.encoder.add_module('conv2', nn.Conv2d(hid_channels, code_channels, kernel_size))\n",
    "        self.encoder.add_module('logits', nn.Sigmoid())\n",
    "        \n",
    "        self.decoder.add_module('Transpose conv1', nn.ConvTranspose2d(code_channels, hid_channels, kernel_size, stride))\n",
    "        self.decoder.add_module('activation', activation)\n",
    "        self.decoder.add_module('upsampling', nn.UpsamplingBilinear2d(scale_factor=pooling))\n",
    "        self.decoder.add_module('Transpose conv2', nn.ConvTranspose2d(hid_channels, in_channels, kernel_size, stride))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def to_intensities(self, logits: torch.Tensor, x_range: Tuple[float, float] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode logits to proper pixel intensities according to assumptions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : torch.Tensor\n",
    "            Logits to be decoded by this auto-encoder.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor\n",
    "            Decoded image with pixel intensities in [0, 1].\n",
    "        \"\"\"\n",
    "        if self.assumption is None:\n",
    "            return logits\n",
    "        elif self.assumption == \"gauss\":\n",
    "            if x_range is not None:\n",
    "                logits.clamp_(*x_range)\n",
    "            return (logits - logits.min()) / (logits.max() - logits.min())\n",
    "        elif self.assumption == \"truncated\":\n",
    "            return logits.clamp_(0, 1)\n",
    "        elif self.assumption == \"binomial\":\n",
    "            return torch.sigmoid(logits)\n",
    "        elif self.assumption == \"binary\":\n",
    "            return (logits > 0).float()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"no implementation for assumption '{self.assumption}', feel free to add your own!\"\n",
    "            )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Reconstruct inputs to proper images. \"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return self.to_intensities(logits, (x.min(), x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dfc855d5883c17e907050ea3d684d10",
     "grade": true,
     "grade_id": "cell-44340745bc0c7a26",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "ae = AutoEncoder(1, 1, 1, 3)\n",
    "assert len(ae.encoder) > 3, (\n",
    "    \"ex3: encoder has too little modules (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba58c532ec21f76cd843edcf077f5ebb",
     "grade": true,
     "grade_id": "cell-90c4a648100da957",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "ae = AutoEncoder(1, 1, 1, 3)\n",
    "assert len(ae.decoder) > 3, (\n",
    "    \"ex3: decoder has too little modules (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79e6666b7bd304d01394ddd181d0eb00",
     "grade": true,
     "grade_id": "cell-5ae408de99c7f8ca",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "ae = AutoEncoder(1, 16, 8, 5)\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = ae(x)\n",
    "assert x.shape == y.shape, (\n",
    "    f\"ex3: reconstructed input has different shape: {tuple(y.shape)} (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd9b61475fccf5264f1f8bf8c740bf5e",
     "grade": true,
     "grade_id": "cell-74bfdd0982e1e9f0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 4: Auto-Encoding (2 points)\n",
    "\n",
    "Of course, training an auto-encoder takes more than just a nice architecture.\n",
    "The loss function as well as the pre-processing of data will also affect the results.\n",
    "There is no right or wrong as long as you get the desired result.\n",
    "However, we can make certain assumptions to guide the search for a good combination.\n",
    "After all, all components have to work together to get nice reconstructions.\n",
    "Can you spot/explain the difference in results for different assumptions?\n",
    "\n",
    " > Train the simple auto-encoder from exercise 3 on the AutoMNIST dataset.\n",
    " > Fill out the template code below using the `AutoEncoderTrainer` class,\n",
    " > which has been defined in the \"preamble\" of this assignment.\n",
    " > Your main task will be to choose a loss function and set up pre-processing of the targets.\n",
    " > Use the `assumption` kwarg of the `AutoEncoder` class to indicate your assumptions.\n",
    " > This should also guarantee correct visualisations of the reconstructions.\n",
    " > Make sure that the assumption matches the chosen loss function and pre-processing.\n",
    " > Feel free to add assumptions in the `reconstruct` method if you feel something is missing.\n",
    "\n",
    "**Hint:** A few `assumption` options are provided in the documentation of `AutoEncoder.__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a251d8bbcc5f892d098c0af562a500c0",
     "grade": false,
     "grade_id": "cell-1a9420b012136606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = AutoMNIST(\n",
    "    data_root,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.1307, ), (.3081, )),\n",
    "    ]),\n",
    "\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.1307, ), (.3081, )),\n",
    "    ]),# None, # TODO: pre-processing\n",
    "    download=True\n",
    ")\n",
    "loss = nn.MSELoss().to(device) \n",
    "auto_encoder = AutoEncoder(\n",
    "    1, 64, 16, 5, activation=nn.ELU(), \n",
    "    assumption = 'gauss',  # TODO: assumption\n",
    ").to(device)\n",
    "trainer = AutoEncoderTrainer(\n",
    "    model=auto_encoder,\n",
    "    criterion=loss,  # TODO: loss function\n",
    "    optimiser=optim.Adamax(auto_encoder.parameters()),\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# consider 1000 images in data as training data\n",
    "# data = torch.utils.data.Subset(data, range(1000))\n",
    "\n",
    "# print(len(data))\n",
    "# train_data = DataLoader(data, batch_size=16, shuffle=True)\n",
    "# train_loss = trainer.train(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "204001b309c814bed6320db5b9bac196",
     "grade": true,
     "grade_id": "cell-b6b8e706a81321cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert len(data.transform.transforms) == 2, (\n",
    "    \"ex4: input transform should not be altered! (-1 point)\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    x_in, x_out = data[0]\n",
    "    assert torch.is_tensor(x_in) and torch.is_tensor(x_out), (\n",
    "        \"ex4: pre-processing does not produce tensors (-1 point)\"\n",
    "    )\n",
    "except (TypeError, RuntimeError):\n",
    "    raise AssertionError(\n",
    "        \"ex4: pre-processing is broken (-1 point)\"\n",
    "    ) from None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ea500e244c02ad83a4193d7851f4520",
     "grade": true,
     "grade_id": "cell-e15fb12fe6ad77c5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x_rec = auto_encoder.reconstruct(torch.randn(16, 1, 28, 28).to(device))\n",
    "assert x_rec.min() >= 0 and x_rec.max() <= 1, (\n",
    "    \"ex4: invalid range for reconstructed values: \"\n",
    "    f\"[{x_rec.min().item(), x_rec.max().item()}] (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - avg loss: 0.983670 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x56>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - avg loss: 0.579345 \n",
      "Epoch  0 - avg loss: 0.195152 \n",
      "Epoch  0 - avg loss: 0.149415 \n",
      "Epoch  0 - avg loss: 0.125352 \n",
      "Epoch  0 - avg loss: 0.108348 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x56>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - avg loss: 0.095647 \n",
      "Epoch  0 - avg loss: 0.085902 \n",
      "Epoch  0 - avg loss: 0.078268 \n",
      "Epoch  0 - avg loss: 0.072259 \n",
      "Epoch  0 - avg loss: 0.067337 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x56>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check\n",
    "loader = DataLoader(data, batch_size=256, shuffle=True, num_workers=4)\n",
    "trainer.train(loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoders\n",
    "\n",
    "Variational auto-encoders (VAEs) extend auto-encoders in a probabilistic way.\n",
    "Although this might sound complicated, it only requires a few modifications to make an auto-encoder variational.\n",
    "First of all, the latent space is regularised to stay close to a (standard normal) distribution.\n",
    "Secondly, VAEs do not produce specific codes, but rather a distribution of codes.\n",
    "In practice this is done by directly mapping inputs to the parameters of a distribution.\n",
    "\n",
    "The main advantage of this approach, is that the the distribution of the latent codes remains well under control.\n",
    "As a result, we can sample from this latent distribution and decode these sampled codes to generate new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 5: Variational Auto-Encoder (4 points)\n",
    "\n",
    "VAEs predict the distribution parameters of the latent space,\n",
    "rather than a specific code in the latent space.\n",
    "In the case of a Gaussian latent distribution, \n",
    "this means that the encoder produces a mean and log-variance for every input sample.\n",
    "By predicting the log-variance, the variance is guaranteed to be positive.\n",
    "\n",
    "Because the codes of a VAE are distributions, it is not possible to directly decode them back to images.\n",
    "Instead, specific codes have to be sampled from the latent distribution before decoding.\n",
    "This is the main difference, architecturally, between AEs and VAEs.\n",
    "The main difference during training is the regularisation.\n",
    "This is what makes VAEs especially interesting for generating new images.\n",
    "\n",
    " > Implement the `VariationalAutoEncoder` class.\n",
    " > The `sample` method should sample a latent vector from a Gaussian with given mean and log-variance. \n",
    " > Note that the `sample` method for a VAE must be differentiable!\n",
    " > In addition to the regular output, \n",
    " > the `forward` method should also return the distribution parameters.\n",
    " > These can then be used to compute the regularisation loss,\n",
    " > which should be implemented in the `regularisation` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a10bc39ee1564ad136f7f81142aa3d4",
     "grade": false,
     "grade_id": "cell-b05bd9cbbe066071",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    \"\"\" Variational auto-encoder with standard normal prior. \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(mean: torch.Tensor, log_var: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sample randomly from the latent space in a differentiable manner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : (N, code_features) Tensor\n",
    "            Mean(s) for sampling new images.\n",
    "        log_var : (N, code_features) Tensor\n",
    "            Logarithm of variance(s) for sampling new images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample : (N, code_features) Tensor\n",
    "            The randomly generated sample in latent space.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        std = torch.sqrt(torch.exp(log_var))\n",
    "        epsilon = torch.normal(0,1, size = mean.shape)# Noise which is standard gaussian distribution\n",
    "        z = mean + std*epsilon #(Sampled new distribution).\n",
    "        return z\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def regularisation(parameters: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Compute regularisation loss.\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters : (Tensor, Tensor)\n",
    "            Mean and log_var predictions from forward pass.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        kl_div : Tensor\n",
    "            The Kullback-Leibler divergence between a standard normal\n",
    "            and the Gaussian distribution specified by `parameters`.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        # log_var = log(sigma^2) ---->>> sigma^2 = exp(log_var)\n",
    "        kl_div = -0.5 * torch.sum(1 + parameters[1] - torch.exp(parameters[1]) - parameters[0]*parameters[0] )   # -0.5(1+log(sigma^2) - sigma^2 - mean^2)\n",
    "        return kl_div\n",
    "\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module, \n",
    "                 assumption: str = None):\n",
    "        super().__init__()\n",
    "        self.assumption = assumption\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The inputs to the forward pass.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y : torch.Tensor\n",
    "            The logits computed from `x`\n",
    "        parameters : tuple of torch.Tensor\n",
    "            The parameters for the latent distribution.\n",
    "            This is used to compute the regularisation.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        x = self.encoder(x)\n",
    "        mean, log_var = torch.chunk(x, 2, dim = 1) # mean and log_var are concatenated in the encoder\n",
    "        z = self.sample(mean, log_var)\n",
    "        y = self.decoder(z)\n",
    "        return y, (mean, log_var)\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, mean: torch.Tensor, log_var: torch.Tensor = 0.,\n",
    "                 y_range: Tuple[float, float] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate one or more new images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : (N, code_features) Tensor\n",
    "            Mean in latent space for generating images.\n",
    "        log_var : (N, code_features) Tensor, optional\n",
    "            Logarithm of variance(s) in latent space for generating images.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        img : (N, in_features) Tensor\n",
    "            Image with the same dimensions as the inputs\n",
    "            and pixel intensities in [0, 1].\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        `sample(mean, log_var)` : sample randomly in latent space.\n",
    "        \"\"\"\n",
    "        device = next(self.encoder.parameters()).device\n",
    "        mean = torch.as_tensor(mean).to(device)\n",
    "        log_var = torch.as_tensor(log_var).to(device)\n",
    "        z = self.sample(mean, log_var)\n",
    "        logits = self.decoder(z)\n",
    "        return AutoEncoder.to_intensities(self, logits, y_range)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Reconstruct inputs to proper images. \"\"\"\n",
    "        logits, _ = self.forward(x)\n",
    "        return AutoEncoder.to_intensities(self, logits, (x.min(), x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbdff66723579226af3acc4a5ac3ba7c",
     "grade": true,
     "grade_id": "cell-65731bfe7f8f0679",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "mean, log_var = torch.ones(1024, 10), torch.zeros(1024, 10)\n",
    "z = VariationalAutoEncoder.sample(mean, log_var)\n",
    "assert z.shape == mean.shape, (\n",
    "    \"ex5: incorrect shape for sampled VAE latents (-1 point)\"\n",
    ")\n",
    "assert torch.allclose(z.mean(0), torch.ones(1), atol=1e-1), (\n",
    "    \"ex5: incorrect mean for sampled VAE latents: \"\n",
    "    f\"'{z.mean(0).abs().max().item():.3f}' (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "004e0b79231868e877b26988590835b3",
     "grade": true,
     "grade_id": "cell-25bdf92c81e34301",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "mean = torch.zeros(1024, 10).requires_grad_(True)\n",
    "log_var = torch.zeros(1024, 10).requires_grad_(True)\n",
    "z = VariationalAutoEncoder.sample(mean, log_var)\n",
    "assert z.requires_grad, (\n",
    "    \"ex5: reparameterisation trick not (correctly) implemented (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90f203882f14e5ebf6d11e7c382a0255",
     "grade": true,
     "grade_id": "cell-39965169b6b1d413",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "mean, log_var = torch.zeros(1, 10), torch.zeros(1, 10)\n",
    "reg = VariationalAutoEncoder.regularisation((mean, log_var))\n",
    "assert torch.allclose(reg, torch.zeros(1)), (\n",
    "    \"ex5: VAE regulariser for perfect parameters is not zero (-1 point)\"\n",
    ")\n",
    "\n",
    "reg = VariationalAutoEncoder.regularisation((mean + 1, log_var))\n",
    "assert torch.allclose(reg, 5 * torch.ones(1)), (\n",
    "    \"ex5: VAE regulariser for parameters with shifted mean is wrong: \"\n",
    "    f\"'{reg.item():.3f}' != 5.0 (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0252f7040fb72d3ef516befd7f8aa08",
     "grade": true,
     "grade_id": "cell-741beeb4f3838d26",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "vae = VariationalAutoEncoder(\n",
    "    encoder=nn.Sequential(\n",
    "        nn.Linear(784, 400), nn.ELU(), nn.Linear(400, 20)\n",
    "    ), decoder=nn.Sequential(\n",
    "        nn.ELU(), nn.Linear(10, 400), nn.ELU(), nn.Linear(400, 784)\n",
    "    )\n",
    ")\n",
    "\n",
    "x = torch.ones(1, 784)\n",
    "y, pars = vae(x)\n",
    "assert y.shape == x.shape, (\n",
    "    f\"ex5: reconstructed input has different shape: {tuple(y.shape)} (-0.5 points)\"\n",
    ")\n",
    "assert pars[0].shape == pars[1].shape, (\n",
    "    f\"ex5: latent distribution parameters have different shapes (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf9e63ae63874283c7cf2bcfe88e1de1",
     "grade": true,
     "grade_id": "cell-fe1614a3ce86818e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(1, 784)\n",
    "y1, _ = vae(x)\n",
    "y2, _ = vae(x)\n",
    "assert not torch.allclose(y1, y2), (\n",
    "    f\"ex5: forward pass seems to be deterministic (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Balance (1 point)\n",
    "\n",
    "In order to incorporate the regularisation loss in our trainer,\n",
    "I provided a (hacky) loss function wrapper: `RegularisedLoss`.\n",
    "This wrapper makes it possible to observe the original loss\n",
    "together with the regularisation loss.\n",
    "This will be useful because regularisation is always a balancing act.\n",
    "Therefore, the goal of this final exercise is to train the VAE\n",
    "in such a way that reconstruction and regularisation losses are balanced.\n",
    "\n",
    " > Train the variational auto-encoder with the regularised loss.\n",
    " > You can copy your reconstruction assumptions and loss function\n",
    " > from the auto-encoder exercise (they are not graded twice).\n",
    " > The key will be to find a reasonable value for the weight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fb9c21583475c6da14c6d1a4810a506",
     "grade": false,
     "grade_id": "cell-59b997323fdb4e3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(\n",
    "    encoder=nn.Sequential(\n",
    "        nn.Flatten(), nn.Linear(784, 400),\n",
    "        nn.ELU(), nn.Linear(400, 40),\n",
    "    ),\n",
    "    decoder=nn.Sequential(\n",
    "        nn.ELU(), nn.Linear(20, 400),\n",
    "        nn.ELU(), nn.Linear(400, 784),\n",
    "        nn.Unflatten(1, (1, 28, 28)),\n",
    "    ),\n",
    "    assumption=None\n",
    ").to(device)\n",
    "regularised_loss = RegularisedLoss(None, vae.regularisation, weight=10e-3)\n",
    "trainer = AutoEncoderTrainer(\n",
    "    model=vae,\n",
    "    criterion=regularised_loss,  \n",
    "    optimiser=optim.Adamax(vae.parameters()),\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84d3858579821c0e05b9f56f6a18a4c1",
     "grade": true,
     "grade_id": "cell-78c32744cd767a94",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert regularised_loss.weight > 0, (\n",
    "    \"ex6: zero weight corresponds to AE training (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ari/anaconda3/envs/dpi/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ari/anaconda3/envs/dpi/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ari/anaconda3/envs/dpi/lib/python3.8/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/ari/anaconda3/envs/dpi/lib/python3.8/shutil.py\", line 722, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/home/ari/anaconda3/envs/dpi/lib/python3.8/shutil.py\", line 720, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-jlazg_co'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# sanity check\u001b[39;00m\n\u001b[1;32m      2\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(data, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(loader, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 143\u001b[0m, in \u001b[0;36mAutoEncoderTrainer.train\u001b[0;34m(self, loader, num_epochs, vis_every)\u001b[0m\n\u001b[1;32m    140\u001b[0m ref_inputs, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(loader))\n\u001b[1;32m    142\u001b[0m \u001b[39m# evaluate random performance\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m errs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(loader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion)\n\u001b[1;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_errs(\u001b[39m0\u001b[39m, errs)\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_display_result(ref_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpi/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m, in \u001b[0;36mAutoEncoderTrainer.evaluate\u001b[0;34m(self, data, metric)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\u001b[39mself\u001b[39m, data: DataLoader, metric: \u001b[39mcallable\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 52\u001b[0m     losses \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(data, metric)]\n\u001b[1;32m     53\u001b[0m     \u001b[39m# print(len(losses))\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\u001b[39mself\u001b[39m, data: DataLoader, metric: \u001b[39mcallable\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 52\u001b[0m     losses \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(data, metric)]\n\u001b[1;32m     53\u001b[0m     \u001b[39m# print(len(losses))\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mAutoEncoderTrainer._forward\u001b[0;34m(self, data, metric)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m data:\n\u001b[1;32m     42\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m     44\u001b[0m     \u001b[39m# print(logits.size()) \u001b[39;00m\n\u001b[1;32m     45\u001b[0m     res \u001b[39m=\u001b[39m metric(logits, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpi/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[34], line 79\u001b[0m, in \u001b[0;36mVariationalAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m     78\u001b[0m mean, log_var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(x, \u001b[39m2\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m# mean and log_var are concatenated in the encoder\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(mean, log_var)\n\u001b[1;32m     80\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z)\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m y, (mean, log_var)\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mVariationalAutoEncoder.sample\u001b[0;34m(mean, log_var)\u001b[0m\n\u001b[1;32m     23\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mexp(log_var))\n\u001b[1;32m     24\u001b[0m epsilon \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m, size \u001b[39m=\u001b[39m mean\u001b[39m.\u001b[39mshape)\u001b[39m# Noise which is standard gaussian distribution\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m z \u001b[39m=\u001b[39m mean \u001b[39m+\u001b[39m std\u001b[39m*\u001b[39;49mepsilon \u001b[39m#(Sampled new distribution).\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "loader = DataLoader(data, batch_size=256, shuffle=True, num_workers=4)\n",
    "trainer.train(loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# sanity check (image generation)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m out1 \u001b[39m=\u001b[39m data_to_image(\u001b[39m*\u001b[39mvae\u001b[39m.\u001b[39;49mgenerate(torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m10\u001b[39;49m, \u001b[39m20\u001b[39;49m)))\n\u001b[1;32m      3\u001b[0m out2 \u001b[39m=\u001b[39m data_to_image(\u001b[39m*\u001b[39mvae\u001b[39m.\u001b[39mgenerate(torch\u001b[39m.\u001b[39mones(\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m)))\n\u001b[1;32m      4\u001b[0m out3 \u001b[39m=\u001b[39m data_to_image(\u001b[39m*\u001b[39mvae\u001b[39m.\u001b[39mgenerate(torch\u001b[39m.\u001b[39mrandn(\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/dpi/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[34], line 110\u001b[0m, in \u001b[0;36mVariationalAutoEncoder.generate\u001b[0;34m(self, mean, log_var, y_range)\u001b[0m\n\u001b[1;32m    108\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    109\u001b[0m log_var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(log_var)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 110\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(mean, log_var)\n\u001b[1;32m    111\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z)\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m AutoEncoder\u001b[39m.\u001b[39mto_intensities(\u001b[39mself\u001b[39m, logits, y_range)\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mVariationalAutoEncoder.sample\u001b[0;34m(mean, log_var)\u001b[0m\n\u001b[1;32m     23\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mexp(log_var))\n\u001b[1;32m     24\u001b[0m epsilon \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m, size \u001b[39m=\u001b[39m mean\u001b[39m.\u001b[39mshape)\u001b[39m# Noise which is standard gaussian distribution\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m z \u001b[39m=\u001b[39m mean \u001b[39m+\u001b[39m std\u001b[39m*\u001b[39;49mepsilon \u001b[39m#(Sampled new distribution).\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# sanity check (image generation)\n",
    "out1 = data_to_image(*vae.generate(torch.zeros(10, 20)))\n",
    "out2 = data_to_image(*vae.generate(torch.ones(10, 20)))\n",
    "out3 = data_to_image(*vae.generate(torch.randn(10, 20)))\n",
    "display(out1, out2, out3, metadata={'width': '100%'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
